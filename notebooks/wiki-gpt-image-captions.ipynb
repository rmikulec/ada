{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: GPT-Powered Wikipedia Image Captions\n",
    "\n",
    "Using the `wikipedia` library to scape for articles related to a user question\n",
    "\n",
    "While the model most likely is already trained on wikipedia, making this idea seem useless, the content will be enhanced by using `gpt-4V` preview in order to inject image descriptions in place of images in the articles. The idea is that hopefully `gpt-4` will be able to reference these images then, so that the user can maybe get a better understanding and reassurance to the answer of the question\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "Image descriptions will provide `gpt-4` with the oppotunity of knowing which image to embed into its answer\n",
    "\n",
    "### Experiment\n",
    "\n",
    "5 questions will be run through the standard `gpt-4` and the image-description enchanced `gpt-4`.\n",
    "\n",
    " - Is climate change real?\n",
    " - My church says evolution is not real because the bible states that everything was made 5,000 years ago.\n",
    " - Do you have any proof that the earth is flat?\n",
    " - How do we know for sure that humans are causing climate change?\n",
    " - What is a nerve?\n",
    "\n",
    "\n",
    "The results will then be manually reviewed and rated in order to determine if this enhancement helped.\n",
    "\n",
    "Additionaly, it is important to consider the costs of running this, as it will signifanctly increase the number of calls to OpenAI's services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tiktoken\n",
    "import asyncio\n",
    "from time import sleep\n",
    "from openai import AsyncOpenAI as OpenAI\n",
    "from openai import BadRequestError, RateLimitError\n",
    "\n",
    "try:\n",
    "    import wikipedia\n",
    "except:\n",
    "    subprocess.run([\"pip\", \"install\", \"wikipedia\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this notebook, the new class will be introduced that extends the original `WikipediaPage`. The new class will have a new property called `image_descriptions` that will asyncronsly generate image captions using OpenAI.\n",
    "\n",
    "For now, only the first Wikipedia result will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiPage(wikipedia.WikipediaPage):\n",
    "\n",
    "    def __init__(self, openai_client: OpenAI, **kwargs):\n",
    "        self.openai_client = openai_client\n",
    "        self.openai_responses = []\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    async def _get_image_caption(self, image_url: str) -> str:\n",
    "        try:\n",
    "            response = await self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4-vision-preview\",\n",
    "                messages=[\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                        {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url,\n",
    "                        },\n",
    "                        },\n",
    "                    ],\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=100,\n",
    "                )\n",
    "            self.openai_responses.append(response)\n",
    "            return response.choices[0].message.content\n",
    "        except BadRequestError:\n",
    "            return f\"Image captioning request failed for image at {image_url}\"\n",
    "\n",
    "    \n",
    "    def _check_image(self, image_url) -> bool:\n",
    "        image_type = image_url.split('.')[-1]\n",
    "        return image_type.lower() in ['png', 'jpeg', 'gif', 'webp']\n",
    "\n",
    "    async def _get_image_captions(self, n_images:int = 5):\n",
    "        images = [\n",
    "            image\n",
    "            for image in self.images\n",
    "            if self._check_image(image)\n",
    "        ]\n",
    "        if n_images:\n",
    "            images = images[0:n_images]\n",
    "\n",
    "        captions = await asyncio.gather(*[self._get_image_caption(image_url=image) for image in images])\n",
    "\n",
    "        return captions, images\n",
    "    \n",
    "    @property\n",
    "    async def image_descriptions(self):\n",
    "        image_str = \"\"\n",
    "        captions, images = await self._get_image_captions(n_images=3)\n",
    "        for caption, image in zip(captions, images):\n",
    "            image_str+=f\"URL: {image}\\nCaption: {caption} \\n\\n\"\n",
    "        return image_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a quick test to make sure that the image descriptions are loading properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://upload.wikimedia.org/wikipedia/commons/5/53/2017_Global_warming_attribution_-_based_on_NCA4_Fig_3.3.png\n",
      "Caption: This image contains two graphs related to climate science.\n",
      "\n",
      "1. The top graph, labeled \"Observed global warming,\" presents a time series from roughly 1880 to after 2010 showing a trend of increasing global temperatures over time. The y-axis measures temperature change in degrees Fahrenheit, ranging from -1°F to +2°F, and the x-axis represents the years. The graph shows a clear upward trend in temperatures, indicating a rise in global average temperatures.\n",
      "\n",
      "2. The bottom graph, labeled \n",
      "\n",
      "URL: https://upload.wikimedia.org/wikipedia/commons/b/bd/20200327_Climate_change_deniers_cherry_picking_time_periods.gif\n",
      "Caption: The image is a scatter plot illustrating the change in global average temperature over time, from around 1850 to the present day. Each dot represents the temperature anomaly (the deviation from a reference temperature average) for a specific year. The vertical axis (y-axis) shows the temperature change in degrees Celsius, ranging from -0.5°C to 1.0°C, while the horizontal axis (x-axis) represents the years from 1850 to approximately 2025.\n",
      "\n",
      "The data points are \n",
      "\n",
      "URL: https://upload.wikimedia.org/wikipedia/commons/6/66/Climate_change_icon.png\n",
      "Caption: This image appears to show a stylized representation of planet Earth with what looks like flames or fiery elements coming off from one side. The depiction of Earth is not realistic or detailed but uses solid blocks of color to signify landmasses and oceans. The flames are likely meant to symbolize something like heat or burning, possibly referencing to global warming or another concept related to the Earth's environment. The image has some visual artifacts indicating that it may have been edited or manipulated. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "page = WikiPage(title=\"Climate change denial\", openai_client=client)\n",
    "print(await page.image_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Is climate change real?\",\n",
    "    \"My church says evolution is not real because the bible states that everything was made 5,000 years ago.\",\n",
    "    \"Do you have any proof that the earth is flat?\",\n",
    "    \"How do we know for sure that humans are causing climate change?\",\n",
    "    \"What is a nerve?\"\n",
    "]\n",
    "\n",
    "def get_page(question):\n",
    "    page_name = wikipedia.search(query=question)[0]\n",
    "    page = WikiPage(\n",
    "        openai_client=client,\n",
    "        title=page_name\n",
    "    )\n",
    "    return page\n",
    "\n",
    "def get_num_tokens(content):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    return len(encoding.encode(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Climate change denial (also global warming denial or climate denial) is the pseudoscientific dismissal or unwarranted doubt that contradicts the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none.Climate change denial includes doubts to the extent of how much climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several social science studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.The conspiracy to undermine public trust in climate science is organized by industrial, political and ideological interests. Climate change denial has been associated with the fossil fuels lobby, the Koch brothers, industry advocates, ultraconservative think tanks and ultraconservative alternative media, often in the United States. More than 90% of papers that are skeptical on climate change originate from right-wing think tanks. Climate change denial is undermining the efforts to act on or adapt to climate change, and exerts a powerful influence on politics of global warming and the manufactured global warming controversy.In the 1970s, oil companies published research which broadly concurred with the scientific community's view on global warming. Since then, for several decades, oil companies have been organizing a widespread and systematic climate change denial campaign to seed public disinformation, a strategy that has been compared to the organized denial of the hazards of tobacco smoking by the tobacco industry. Some of the campaigns are even carried out by the same individuals who previously spread the tobacco industry's denialist propaganda.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def standard_call(question):\n",
    "    system_message=\"\"\"\n",
    "You are SciXplain! A new science communicator AI that can answer any question about science!\n",
    "Your goal is to accurately answer user's questions, explain misinterpretations, settle doubts against establised science,\n",
    "but most importantly, maintain a positive vibe to keep people engadged. You want to steer them into learning more, rather\n",
    "than disencouraging.\n",
    "\"\"\"\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":system_message},\n",
    "            {\"role\":\"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def call_with_images(question):\n",
    "    page = get_page(question=question)\n",
    "    # Sleep to avoid RateLimitError\n",
    "    sleep(60)\n",
    "    image_descriptions = await page.image_descriptions\n",
    "    # Check if it is under to token limit\n",
    "    if get_num_tokens(page.content) + get_num_tokens(image_descriptions) > 8_000:\n",
    "        content = page.summary\n",
    "    else:\n",
    "        content = page.content\n",
    "    \n",
    "    if get_num_tokens(content) + get_num_tokens(image_descriptions) > 8_000:\n",
    "        content = f\"This is a page about {page.title}\"\n",
    "    system_message=f\"\"\"\n",
    "You are SciXplain! A new science communicator AI that can answer any question about science!\n",
    "Your goal is to accurately answer user's questions, explain misinterpretations, settle doubts against establised science,\n",
    "but most importantly, maintain a positive vibe to keep people engadged. You want to steer them into learning more, rather\n",
    "than disencouraging.\n",
    "\n",
    "The question will also be searched on Wikipedia, and you will be provieded content from that page, as well as descriptions of some images\n",
    "on the page. Please reference images if applicable by embedding the provided image url into the response. This may help users better understand\n",
    "a topic by seeing a visual aid with the answer.\n",
    "\n",
    "Wikipedia Page Content:\n",
    "{content}\n",
    "\n",
    "Wikipedia Page Image Descriptions:\n",
    "{image_descriptions}\n",
    "\"\"\"\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":system_message},\n",
    "            {\"role\":\"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_responses = await asyncio.gather(*[standard_call(question) for question in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Paw04xDBT2zTglqiooCvZ6oQ on tokens per min (TPM): Limit 10000, Used 2254, Requested 9008. Please try again in 7.572s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m image_responses \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39m[call_with_images(question) \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m questions])\n",
      "\u001b[1;32m/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         content \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is a page about \u001b[39m\u001b[39m{\u001b[39;00mpage\u001b[39m.\u001b[39mtitle\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mYou are SciXplain! A new science communicator AI that can answer any question about science!\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mYour goal is to accurately answer user\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms questions, explain misinterpretations, settle doubts against establised science,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m{\u001b[39;00mimage_descriptions\u001b[39m}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         messages\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m:system_message},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: question}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryan/projects/scixplain/notebooks/data-exploration.ipynb#X25sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/resources/chat/completions.py:1199\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   1153\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m   1154\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1198\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m   1200\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/chat/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1201\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1202\u001b[0m             {\n\u001b[1;32m   1203\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[1;32m   1204\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m   1205\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1206\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m: function_call,\n\u001b[1;32m   1207\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: functions,\n\u001b[1;32m   1208\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1209\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1210\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[1;32m   1211\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1212\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mresponse_format\u001b[39m\u001b[39m\"\u001b[39m: response_format,\n\u001b[1;32m   1213\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[1;32m   1214\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[1;32m   1215\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[1;32m   1216\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m   1217\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtool_choice\u001b[39m\u001b[39m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1218\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m: tools,\n\u001b[1;32m   1219\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m   1220\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[1;32m   1221\u001b[0m             },\n\u001b[1;32m   1222\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1223\u001b[0m         ),\n\u001b[1;32m   1224\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m   1225\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m   1226\u001b[0m         ),\n\u001b[1;32m   1227\u001b[0m         cast_to\u001b[39m=\u001b[39mChatCompletion,\n\u001b[1;32m   1228\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m         stream_cls\u001b[39m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1230\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1474\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1462\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_AsyncStreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1470\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1471\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1472\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39m\u001b[39mawait\u001b[39;00m async_to_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1473\u001b[0m     )\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1275\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     remaining_retries: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1276\u001b[0m         cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1277\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1278\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1279\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1280\u001b[0m         remaining_retries\u001b[39m=\u001b[39mremaining_retries,\n\u001b[1;32m   1281\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1306\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m-> 1306\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m   1307\u001b[0m             options,\n\u001b[1;32m   1308\u001b[0m             cast_to,\n\u001b[1;32m   1309\u001b[0m             retries,\n\u001b[1;32m   1310\u001b[0m             err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1311\u001b[0m             stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1312\u001b[0m             stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1313\u001b[0m         )\n\u001b[1;32m   1315\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[39mawait\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39maread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1356\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1352\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRetrying request to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m.\u001b[39murl, timeout)\n\u001b[1;32m   1354\u001b[0m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1356\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1357\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1358\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1359\u001b[0m     remaining_retries\u001b[39m=\u001b[39mremaining,\n\u001b[1;32m   1360\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1361\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1362\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1306\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m-> 1306\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m   1307\u001b[0m             options,\n\u001b[1;32m   1308\u001b[0m             cast_to,\n\u001b[1;32m   1309\u001b[0m             retries,\n\u001b[1;32m   1310\u001b[0m             err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1311\u001b[0m             stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1312\u001b[0m             stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1313\u001b[0m         )\n\u001b[1;32m   1315\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[39mawait\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39maread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1356\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1352\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRetrying request to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m.\u001b[39murl, timeout)\n\u001b[1;32m   1354\u001b[0m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1356\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1357\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1358\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1359\u001b[0m     remaining_retries\u001b[39m=\u001b[39mremaining,\n\u001b[1;32m   1360\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1361\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1362\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/envs/scix/lib/python3.9/site-packages/openai/_base_client.py:1318\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[39mawait\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39maread()\n\u001b[0;32m-> 1318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mConnectTimeout \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1320\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-Paw04xDBT2zTglqiooCvZ6oQ on tokens per min (TPM): Limit 10000, Used 2254, Requested 9008. Please try again in 7.572s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "image_responses = await asyncio.gather(*[call_with_images(question) for question in questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK ON HOLD\n",
    "\n",
    "Keep hitting a `RateLimitError` when attempting to build the iamge captions. As of 11/20/2023, there are strict limitations with using the gpt-4v preview model, making this experiment difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, standard_res, image_res in zip(questions, standard_responses, image_responses):\n",
    "    print(question)\n",
    "    print(f\"\\nStandard: {standard_res}\")\n",
    "    print(f\"\\nImage Response: {image_res}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_with_images(question, page, image_descriptions):\n",
    "    # Check if it is under to token limit\n",
    "    if get_num_tokens(page.content) + get_num_tokens(image_descriptions) > 8_000:\n",
    "        content = page.summary\n",
    "    else:\n",
    "        content = page.content\n",
    "    \n",
    "    if get_num_tokens(content) + get_num_tokens(image_descriptions) > 8_000:\n",
    "        content = f\"This is a page about {page.title}\"\n",
    "    system_message=f\"\"\"\n",
    "You are SciXplain! A new science communicator AI that can answer any question about science!\n",
    "Your goal is to accurately answer user's questions, explain misinterpretations, settle doubts against establised science,\n",
    "but most importantly, maintain a positive vibe to keep people engadged. You want to steer them into learning more, rather\n",
    "than disencouraging.\n",
    "\n",
    "The question will also be searched on Wikipedia, and you will be provieded content from that page, as well as descriptions of some images\n",
    "on the page. Please reference images if applicable by embedding the provided image url into the response. This may help users better understand\n",
    "a topic by seeing a visual aid with the answer.\n",
    "\n",
    "Wikipedia Page Content:\n",
    "{content}\n",
    "\n",
    "Wikipedia Page Image Descriptions:\n",
    "{image_descriptions}\n",
    "\"\"\"\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":system_message},\n",
    "            {\"role\":\"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"\"\"\n",
    "URL: https://upload.wikimedia.org/wikipedia/commons/5/53/2017_Global_warming_attribution_-_based_on_NCA4_Fig_3.3.png\n",
    "Caption: This image contains two graphs related to climate change:\n",
    "\n",
    "1. The top graph is labeled \"Observed global warming\" and shows a red line graph depicting an upward trend in global temperatures over time. The y-axis is labeled with temperature changes in Fahrenheit (from -1°F to +2°F), and the x-axis represents the timeline from around 1880 to just beyond 2020. The graph demonstrates that the average global temperature has been rising, especially noticeable from the late 20th century onwards \n",
    "\n",
    "URL: https://upload.wikimedia.org/wikipedia/commons/b/bd/20200327_Climate_change_deniers_cherry_picking_time_periods.gif\n",
    "Caption: This image displays a scatter plot graph titled \"Global average temperature change.\" The vertical axis represents temperature change in degrees Celsius, ranging from -0.5 °C to 1.0 °C, and the horizontal axis represents years, ranging from 1850 to just beyond the year 2020. Each dot on the graph represents a data point that correlates a particular year with the global average temperature change for that year relative to a baseline.\n",
    "\n",
    "The data points are colored: the majority are red, \n",
    "\n",
    "URL: https://upload.wikimedia.org/wikipedia/commons/6/66/Climate_change_icon.png\n",
    "Caption: The image contains a stylized representation of the Earth with a partial, brightly colored flame-like overlay at the right side. The Earth is depicted in blue and green, indicating water and landmasses, respectively. The flame overlay is in shades of orange and yellow, with an outer frayed edge that has some digital artifacting, possibly due to the image manipulation. The graphic appears to be composited or altered, and it could potentially symbolize concepts such as global warming or climate change given the juxtap \n",
    "\"\"\"\n",
    "\n",
    "print(await call_with_images(question=questions[0], page=page, image_descriptions=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = wikipedia.page(\"Dinosaur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(test_page.html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
